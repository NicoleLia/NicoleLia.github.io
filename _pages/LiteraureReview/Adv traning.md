---
permalink: /adversarial-robustness
title: ''
excerpt: ''
author_profile: true
redirect_from:
---

# Adversarial Robustness

## Robustness & Attack

### Evaluate

- ROBUSTNESS[1] - find the smallest perturbation in the input data, the larger the perturbation the more robust it is

  $$
  \text{ROBUSTNESS} \begin{cases}
  \text{minimal perturbation:} & \delta_{\text{min}} = \arg\min_{\delta} \|\delta\| \text{ subject to } F(x + \delta) \neq y. \\
  \text{robustness:} & r(x, F) = \|\delta_{\text{min}}\|. \\
  \text{global robustness:} & \rho(F) = \mathbb{E}_{x \sim D} [r(x, F)].
  \end{cases}
  $$

  1. if given a classifier F and a set of data (x,y), find a minimum perturbation $\delta$ such that $\delta + x ≠ y$;
  2. for a single input, the smallest perturbation norm is $\delta_{min}$;
  3. is defined globally over the dataset by computing the average of the robustness over the entire dataset.

- LOSS[1] - it is to find the maximum Loss within the linear domain $\epsilon$, and the lower the Loss, the more robust it is.

  $$
  \text{Loss} \begin{cases}
  \text{Most-Adversarial Example:} & \quad x_{\text{adv}} = \arg\max_{x'} \mathcal{L}(x', F) \quad \text{subject to} \quad \|x' - x\| \leq \epsilon \\
  \text{Adversarial Loss:} & \quad \mathcal{L}_{\text{adv}}(x) = \mathcal{L}(x_{\text{adv}}, F) = \max_{\|x' - x\| \leq \epsilon} \mathcal{L}(\theta, x', y) \\
  \text{Global Adversarial Loss:} & \quad R_{\text{adv}}(F) = \mathbb{E}_{x \sim D} \left[ \max_{\|x' - x\| \leq \epsilon} \mathcal{L}(\theta, x', y) \right]
  \end{cases}
  $$

(pov: it's really essentially about finding a noise, in relation to RLHF - adding tiny noises is unrecognisable by humans, but humans can correct some of the bias generated by the Agent)

### Attack

![alt text](/_pages/LiteraureReview/image/image.png)

#### White Attack

1. L-BFGS: Adversarial example has the minimumå L2 norm and respect to the original image and predicts incorrectly
2. **FGSM: find the maximum perturbation of the minimum gradient [2]**
3. DeepFool: find the closest adversarial example to the original data point in each iteration, i.e., linearised decision boundary
4. BIM/PGD (Basic Iterative Method/Projected Gradient Descent, PGD): add small perturbations to the image until iteration to generate an adversarial sample
5. Carlini & Wagner: minimise adversarial samples and L2 norm
6. ...

#### Black Attack

This is a Query-based attack that sends inputs to observe the model's output feedback

#### Poisoning Attacks

Backdoor is a poisoning attack, but how to undo the poisoning?
Is it possible to undo fine tuning on a poisoned model?

#### Semi-white (Grey) box Attack

GAN[3]

There's a lot more, like physical attacks etc etc, more on that when I have time

### Defense

There are currently three approaches to defense DNN: Gradient Masking - , Robust Optimization, Adversarial Example Detection[1]

![alt text](/_pages/LiteraureReview/image/image-1.png)

#### Gradient Masking

Fuzzy model gradients to reduce sensitivity to input data

- DEFENSIVE DISTILLATION
  Train a teacher model, then use the output of the teacher model to train a student model (why is this so relevant to Distillation and Amplification? Maybe DA was extended based on this? And doesn't the distillation process lead to performance degradation? Because students don't always learn everything from the teacher?)
- SHATTERED GRADIENTS
- STOCHASTIC/RANDOMIZED GRADIENTS
  Introducing random noise into model training and inference - the goal is to make it impossible to attack because others can't figure out the gradient.
- EXPLODING & VANISHING
  It turns the model into an extremely deep neural network.
  It's amazing how a flaw becomes a defense...?

#### Certified Defenses

The focus is on identification, checking whether the sample is benign or malignant before entering it, and processing the data.

#### Robust Optimization

- PROVABLE DEFENSES

  - Lower Bound of Minimal Perturbation
  - Upper Bound of Adversarial Loss

- REGULARIZATION METHODS

  - Weight Decay: Reduce the model weights mainly through L2 regularisation to control the model complexity and reduce overfitting.
  - Dropout: Randomly discard some neurons during the training process.

- ADVERSARIAL (RE)TRAINING

  - FGSM：
    \[ x' = x + \epsilon \cdot \text{sign}(\nabla_x J(\theta, x, y)) \]
    $x$ is the original input, $x'$ is the adversarial sample, $\epsilon$ is the parameter controlling the magnitude of the perturbation, $\nabla_x J(\theta, x, y)$ is the gradient of the loss function $J$ with respect to the input $x$, $\theta$ is the model parameter, $y$ is the true label.

  - PGD：iteration of FGSM
    \[ x^{t+1} = \Pi*{x+S}(x^t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^t, y))) \]
    $x^t$ is the current adversarial sample in the iterative process, $\alpha$ is the step size of each step, and $\Pi*{x+S}$ is the operation of projecting the perturbation $x^t + \alpha \cdot \text{sign}(\nabla_x J(\theta, x^t, y))$ back to the set of perturbations $S$ to ensure that the perturbations are within $\epsilon $ range.

  - Ensemble Adversarial Training...
  - Accelerate Adversarial Training

(POV: Maybe Robustness Optimization is better if it's optimisation? Because this one enhances the model's resistance to perturbations and noise in the input data to boost robustness)

[1] Xu, H., Ma, Y., Liu, H. C., Deb, D., Liu, H., Tang, J. L., & Jain, A. K. (2020). Adversarial attacks and defenses in images, graphs and text: A review. International journal of automation and computing, 17, 151-178.
[2] Goodfellow, I. J., Shlens, J., & Szegedy, C. (2014). Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572.
[3] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., WardeFarley, D., Ozair, S., Courville, A., and Bengio, Y. Generative
adversarial nets. In Advances in neural information processing
systems, pp. 2672–2680, 2014a.
