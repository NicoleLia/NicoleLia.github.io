---
permalink: /blog
title: ''
excerpt: ''
author_profile: true
redirect_from:
---

# ðŸ‘“ Reading

- Literature Reviews
- Alignment Challenges
  - Language models to generate outputs that may not align with specific human preferences or follow instructions accurately.
    - [Training Language Models to Follow Instructions with Human Feedback](/_pages/AlignmentChallenges/HumanPreferances/index.md)
