---
permalink: /
title: ''
excerpt: ''
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a Master of IT student at the University of Auckland. My research interest includes machine learning and multimodal learning. I am also a full-stack developer and am interested in crawling.

# ðŸ“– Educations

- _2016.09 - 2020.06_, Harbin Engineering University, Bachlor of Computer Science and Technology.
- _2023.11 - 2025.03 (now)_, University of Auckland, Master of Information Technology.

# ðŸ’» Work Experience

- _2020.07 - 2021.07_, Front-End Web Developer, Shenzhen Xiaoxi Streaming Technology Co, China.
- _2021.10 - 2023.03_, Front-End Web Developer, Shenzhen Jiangren Network Technology Co, China.

# ðŸ‘“ Reading

- Fundamental Theory

  - Fundamental Modal

    - <details>
        <summary>Transformer</summary>
        <b>Aim:</b>
        <div></div>
        <b>Pros:</b>
        <div></div>
        <b>Cons:</b>
        <div></div>
      </details>

    - <details>
        <summary>Bert</summary>
        <b>Aim:</b>
        <div></div>
        <b>Pros:</b>
        <div></div>
        <b>Cons:</b>
        <div></div>
      </details>

  - Multimodal Modal

- Alignment

- VLM
  - Macroscopic Perspective - <details>
    <summary>Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs</summary>
    <b>Aim:</b>
    <div>Let AI learn to design multimodal models with a visual focus that enhances visual capabilities to drive AI understanding and decision-making. The focus shifts from scaling LLMs to enhancing visual representations.</div>
    <b>Contribution:</b>
    <div>CV-Bench: presents a new vision-centered benchmark for more accurate evaluation of visual perception in multimodal large language models.</div>
    <div>
    Spatial Vision Aggregator (SVA): A dynamic and spatially aware connector is proposed that integrates high-resolution visual features while reducing the number of tokens required for processing. Improved information integration efficiency and accuracy are achieved.</div>
    <b>Future research directions: pure visual models and their integration into MLLM. And these models should utilize large-scale datasets more effectively and maintain the advantages of a strong visual foundation.</b>
    <b>Pros:</b>
    <div></div>
    <b>Cons:</b>
    <div></div>

</details>
